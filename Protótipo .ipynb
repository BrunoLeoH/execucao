{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d08e182",
   "metadata": {},
   "source": [
    "# Estruturação do Protótipo – Testes Comparativos de Modelos e Bancos de Vetores\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Para garantir uma escolha fundamentada das ferramentas a serem utilizadas no sistema de controle de qualidade, serão realizados testes com foco na eficiência, acurácia, tempo de resposta e complexidade de implementação dos componentes centrais da arquitetura baseada em vetores.\n",
    "\n",
    "## 1. Teste de Modelos de Vetores Locais (Gensim vs Hugging Face)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Nesta primeira fase, o objetivo é comparar o desempenho de dois frameworks de vetorização local, Gensim e Hugging Face Transformers. Para isso, será criado um questionário fictício, composto por pares de perguntas e respostas representativas dos questionários reais utilizados em estudos epidemiológicos, permitindo a execução rápida dos testes sem a necessidade de lidar com dados sensíveis ou exigências de anonimização. O teste seguirá os seguintes critérios:\n",
    "\n",
    "- Precisão na similaridade semântica: capacidade do modelo de identificar a correspondência correta entre perguntas e respostas;\n",
    "- Tempo de processamento médio por par;\n",
    "- Carga computacional (uso de CPU/RAM);\n",
    "- Facilidade de implementação e ajuste fino;\n",
    "- Robustez em textos com variações sintáticas e gramaticais.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Com base nesses dados, será possível definir qual modelo oferece o melhor custo-benefício para ser utilizado como vetorizador no fluxo principal do sistema.\n",
    "\n",
    "## 2. Teste de Bancos de Vetores (Elasticsearch vs GCP Vector Search)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Com os vetores gerados a partir do modelo selecionado na etapa anterior, será iniciado o teste comparativo entre dois bancos de vetores: Elasticsearch com extensão para vetores e Google Cloud Vector Search. Esse teste avaliará:\n",
    "\n",
    "- Velocidade de indexação e consulta;\n",
    "- Precisão na busca de similaridade vetorial;\n",
    "- Facilidade de integração com o restante do sistema;\n",
    "- Custo de operação e infraestrutura;\n",
    "- Complexidade de configuração e manutenção.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Ambos os bancos serão alimentados com o mesmo conjunto de vetores e submetidos a consultas idênticas de similaridade. Os resultados obtidos (ex: top-N mais similares) serão comparados tanto qualitativamente quanto quantitativamente, considerando se há divergências relevantes entre os dois mecanismos.\n",
    "\n",
    "## 3. Objetivo da Comparação\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; A partir dos dois testes será possível:\n",
    "\n",
    "- Selecionar a melhor combinação modelo + banco para a implementação final do sistema;\n",
    "- Validar a viabilidade técnica da arquitetura baseada em vetores, assegurando que ela atenda aos requisitos de precisão, desempenho, privacidade e escalabilidade;\n",
    "- Documentar as decisões tomadas e os resultados obtidos, formando uma base sólida para a etapa de integração e validação final com dados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d6da7e",
   "metadata": {},
   "source": [
    "# 1. Comparação entre Gensim vs Hugging Face\n",
    "\n",
    "### Gensim : \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58b7c57e-1c75-4362-b525-e70ffeba10ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensimNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading gensim-4.4.0-cp313-cp313-win_amd64.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\jwlia\\anaconda3\\lib\\site-packages (from gensim) (2.1.3)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\jwlia\\anaconda3\\lib\\site-packages (from gensim) (1.15.3)\n",
      "Collecting smart_open>=1.8.1 (from gensim)\n",
      "  Downloading smart_open-7.5.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: wrapt in c:\\users\\jwlia\\anaconda3\\lib\\site-packages (from smart_open>=1.8.1->gensim) (1.17.0)\n",
      "Downloading gensim-4.4.0-cp313-cp313-win_amd64.whl (24.4 MB)\n",
      "   ---------------------------------------- 0.0/24.4 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.5/24.4 MB 2.7 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 1.3/24.4 MB 3.6 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 1.8/24.4 MB 3.6 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 2.6/24.4 MB 3.3 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 3.4/24.4 MB 3.4 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 4.5/24.4 MB 3.6 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 5.0/24.4 MB 3.6 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 5.8/24.4 MB 3.5 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 6.6/24.4 MB 3.5 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 7.3/24.4 MB 3.5 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 8.1/24.4 MB 3.6 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 8.9/24.4 MB 3.6 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 10.0/24.4 MB 3.7 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 11.0/24.4 MB 3.8 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 12.1/24.4 MB 3.8 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 12.8/24.4 MB 3.8 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 14.2/24.4 MB 3.9 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 14.9/24.4 MB 4.0 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 16.3/24.4 MB 4.1 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 17.3/24.4 MB 4.1 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 18.4/24.4 MB 4.1 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 19.1/24.4 MB 4.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 20.2/24.4 MB 4.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.2/24.4 MB 4.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 22.3/24.4 MB 4.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.1/24.4 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  23.9/24.4 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.4/24.4 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.4/24.4 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.4/24.4 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.4/24.4 MB 3.8 MB/s eta 0:00:00\n",
      "Downloading smart_open-7.5.0-py3-none-any.whl (63 kB)\n",
      "Installing collected packages: smart_open, gensim\n",
      "\n",
      "   ---------------------------------------- 0/2 [smart_open]\n",
      "   ---------------------------------------- 0/2 [smart_open]\n",
      "   ---------------------------------------- 0/2 [smart_open]\n",
      "   ---------------------------------------- 0/2 [smart_open]\n",
      "   ---------------------------------------- 0/2 [smart_open]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   ---------------------------------------- 2/2 [gensim]\n",
      "\n",
      "Successfully installed gensim-4.4.0 smart_open-7.5.0\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1123fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recurso 'punkt' não encontrado. Baixando...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jwlia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "\n",
    "\n",
    "# Isso só precisa ser feito uma vez. Se o recurso já estiver instalado,\n",
    "# o NLTK irá ignorar a chamada.\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    print(\"Recurso 'punkt' não encontrado. Baixando...\")\n",
    "    nltk.download('punkt')\n",
    "\n",
    "# Agora pode prosseguir com seu código que usa o tokenizador\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "888819fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baixar o tokenizer do NLTK se ainda não tiver\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except nltk.downloader.DownloadError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Função para pré-processar o texto:\n",
    "    - Converter para minúsculas\n",
    "    - Remover caracteres especiais e números\n",
    "    - Tokenizar o texto em palavras\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34c13a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Ler o arquivo de texto\n",
    "# Supondo que você tenha um arquivo chamado 'meu_texto.txt'\n",
    "file_path1 = 'textos/Texto_grande1.txt'\n",
    "with open(file_path1, 'r', encoding='utf-8') as file:\n",
    "    text_data1 = file.read()\n",
    "    \n",
    "file_path2 = 'textos/Texto_grande2.txt'\n",
    "with open(file_path2, 'r', encoding='utf-8') as file:\n",
    "    text_data2 = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01da8ba4",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\jwlia/nltk_data'\n    - 'C:\\\\Users\\\\jwlia\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\jwlia\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\jwlia\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\jwlia\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 2. Pré-processar o texto\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# A Gensim precisa de uma lista de listas de palavras\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m processed_text1 \u001b[38;5;241m=\u001b[39m [preprocess_text(text_data1)]\n\u001b[0;32m      5\u001b[0m processed_text2 \u001b[38;5;241m=\u001b[39m [preprocess_text(text_data2)]\n",
      "Cell \u001b[1;32mIn[5], line 16\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     14\u001b[0m text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mlower()\n\u001b[0;32m     15\u001b[0m text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[^a-zA-Z\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[1;32m---> 16\u001b[0m tokens \u001b[38;5;241m=\u001b[39m word_tokenize(text)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    145\u001b[0m     ]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m _get_punkt_tokenizer(language)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PunktTokenizer(language)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_lang(lang)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m find(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt_tab/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\jwlia/nltk_data'\n    - 'C:\\\\Users\\\\jwlia\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\jwlia\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\jwlia\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\jwlia\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# 2. Pré-processar o texto\n",
    "# A Gensim precisa de uma lista de listas de palavras\n",
    "processed_text1 = [preprocess_text(text_data1)]\n",
    "\n",
    "processed_text2 = [preprocess_text(text_data2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90748c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Treinar o modelo Word2Vec\n",
    "# size: dimensão do vetor (ex: 100)\n",
    "# window: distância máxima entre as palavras\n",
    "# min_count: ignora palavras com frequência menor que o valor\n",
    "model1 = Word2Vec(sentences=processed_text1, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "model2 = Word2Vec(sentences=processed_text2, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a41da107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vetor da palavra 'diabetes'no preimeiro texto:\n",
      "[ 0.00116247  0.00669514  0.00995835  0.00917092 -0.00796529  0.0061822\n",
      " -0.00556052 -0.00048948  0.00033135  0.00646399  0.0044472   0.00431627\n",
      "  0.00934619  0.00047679 -0.00610771 -0.00668483  0.0064551  -0.0057163\n",
      " -0.0028707   0.00346594 -0.002227   -0.00609086 -0.00202447  0.00108997\n",
      "  0.0018904   0.00606529 -0.00540146  0.00288094  0.00698573  0.00233989\n",
      "  0.00572223 -0.0047968   0.00620538 -0.00770982  0.00332909 -0.00893255\n",
      " -0.00271893 -0.0094422  -0.00176624 -0.00594257 -0.00383518  0.00076114\n",
      "  0.00262787 -0.00136894 -0.00793076 -0.00604055  0.00082898 -0.00432166\n",
      " -0.00920345 -0.00063056  0.00695078  0.00585912 -0.00973514  0.00321575\n",
      " -0.0061601  -0.00900602  0.00018274 -0.00043589 -0.00738735 -0.00623154\n",
      " -0.00249949  0.00722684 -0.00735943  0.00739113 -0.00095744  0.00128328\n",
      "  0.00983922  0.00508942 -0.00406008  0.00407286  0.00331716  0.00644615\n",
      "  0.0003032  -0.00430758  0.0015997  -0.00521035  0.00131328  0.00499725\n",
      "  0.00487305  0.00938699 -0.00775457 -0.00546418  0.00633111  0.00160512\n",
      " -0.00667907  0.00091525  0.00271448 -0.00216504 -0.00457563  0.00503228\n",
      "  0.01007313 -0.00706412 -0.00029071 -0.00261147 -0.00607228 -0.00120984\n",
      " -0.00516726  0.0087987  -0.00569318  0.00354627]\n",
      "Dimensão do vetor: 100\n"
     ]
    }
   ],
   "source": [
    "# 4. Obter o vetor de uma palavra específica\n",
    "word_vector = model1.wv['diabetes']\n",
    "print(f\"Vetor da palavra 'diabetes'no preimeiro texto:\\n{word_vector}\")\n",
    "print(f\"Dimensão do vetor: {len(word_vector)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75355108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vetor da palavra 'diabetes'no segundo texto:\n",
      "[-9.6380347e-03  8.9673465e-03  4.1670455e-03  9.2724599e-03\n",
      "  6.6759409e-03  2.8040418e-03  9.8745413e-03 -4.1942401e-03\n",
      " -6.8763345e-03  4.0967651e-03  3.7600102e-03 -5.7528471e-03\n",
      "  9.7121568e-03 -3.5238224e-03  9.5383823e-03  8.7639410e-04\n",
      " -6.2691588e-03 -2.0502349e-03 -7.4083381e-03 -3.1940669e-03\n",
      "  1.0058088e-03  9.4940392e-03  9.2958752e-03 -6.6911154e-03\n",
      "  3.3851967e-03  2.2989814e-03 -2.5143391e-03 -9.2891362e-03\n",
      "  9.4976590e-04 -8.1227766e-03  6.3550202e-03 -5.7360665e-03\n",
      "  5.5401917e-03  9.8324036e-03 -3.1085845e-04  4.7067110e-03\n",
      " -1.7475730e-03  7.3141563e-03  3.8652080e-03 -9.1464175e-03\n",
      " -2.3831292e-03  3.5100786e-03 -1.1393487e-04 -1.1654056e-03\n",
      " -9.1249868e-04 -1.6063573e-03  5.7242339e-04  4.1057109e-03\n",
      " -4.2622429e-03 -3.8027600e-03  8.9254418e-06  2.4184470e-04\n",
      " -1.2595863e-04 -4.8125316e-03  4.2730393e-03 -2.1034500e-03\n",
      "  2.0764091e-03  6.6840486e-04  5.9087332e-03 -6.7958785e-03\n",
      " -6.8743359e-03 -4.4036401e-03  9.4719101e-03 -1.6230860e-03\n",
      " -9.5836921e-03 -4.6162723e-04 -4.4058203e-03  6.1110994e-03\n",
      " -9.6934922e-03  2.9602966e-03 -9.3030389e-03  1.2844446e-03\n",
      "  6.0655545e-03  7.3193647e-03 -7.5148209e-03 -5.9945886e-03\n",
      " -6.7962529e-03 -7.9602636e-03 -9.6717430e-03 -2.0724216e-03\n",
      " -8.4363186e-04 -7.2022411e-03  6.7279902e-03  1.2373406e-03\n",
      "  5.8128587e-03  1.5198258e-03  7.9432927e-04 -7.2916653e-03\n",
      " -2.0870625e-03  4.2711599e-03 -4.9437159e-03  1.2474678e-03\n",
      "  2.8774901e-03 -1.4270390e-03  1.0097236e-02  8.4602917e-03\n",
      "  2.5346978e-03  7.0264484e-03  5.9584300e-03 -5.5520227e-03]\n",
      "Dimensão do vetor: 100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "word_vector = model2.wv['diabetes']\n",
    "print(f\"Vetor da palavra 'diabetes'no segundo texto:\\n{word_vector}\")\n",
    "print(f\"Dimensão do vetor: {len(word_vector)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b1d4ef1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vetor do documento (média dos vetores das palavras no texto 1):\n",
      "[-5.29617537e-04  7.70135957e-04  1.39765078e-04  5.55878505e-04\n",
      "  1.42321631e-04 -1.16600061e-03  9.01830033e-04  2.05199886e-03\n",
      " -1.16814894e-03 -9.79513396e-04  1.56985589e-05 -1.08516915e-03\n",
      " -1.95458531e-04  4.01439203e-04  1.86275953e-04 -8.66785937e-04\n",
      "  8.89081159e-04 -5.15104795e-04 -7.57790811e-04 -2.33665644e-03\n",
      "  4.96209774e-04 -5.06648612e-05  1.57977268e-03 -4.56533424e-04\n",
      " -5.18668501e-04  1.08133550e-04 -7.51901825e-04 -1.12117355e-04\n",
      " -1.04805198e-03  4.73991444e-04  1.21922058e-03 -4.10431356e-04\n",
      "  3.27792513e-04 -1.18291273e-03 -5.10402489e-04  1.10493146e-03\n",
      "  3.05078225e-04 -5.91725286e-04 -7.75955676e-04 -1.22505077e-03\n",
      "  7.32098764e-04 -1.06851745e-03 -7.99787405e-04  3.28298105e-04\n",
      "  1.01290445e-03 -3.64711828e-04 -5.24596253e-04 -7.92251783e-04\n",
      "  5.50458732e-04  3.76215059e-04  7.38859992e-04 -8.77296785e-04\n",
      " -1.22024561e-04 -1.80589574e-04 -8.84129026e-04  2.98518193e-04\n",
      "  2.83718051e-04 -3.45357810e-04 -1.01696968e-03  1.55035086e-04\n",
      "  1.24726766e-05 -2.72335397e-04  1.18278386e-03 -5.16385539e-04\n",
      " -1.25280046e-03  1.23966020e-03  6.06619054e-04  1.17622223e-03\n",
      " -1.72632304e-03  7.21777731e-04 -3.39108577e-04  9.40908387e-04\n",
      "  9.39843943e-04  6.15152530e-04  9.45625361e-04  4.80679009e-04\n",
      "  1.57875213e-04  3.92663613e-04 -6.86693005e-04  3.28015485e-05\n",
      " -9.07906389e-04 -4.92333376e-04 -5.35424391e-04  9.04373534e-04\n",
      " -6.09389623e-04 -6.77109987e-04  7.26927305e-04  5.68863412e-04\n",
      "  9.76536656e-04  1.42358680e-04  1.02443621e-03  7.47221697e-04\n",
      " -1.03205937e-04 -3.24371242e-04  1.64540880e-03  6.27100351e-04\n",
      "  6.44801243e-04 -9.34943033e-04  4.53454384e-04 -1.51244836e-04]\n",
      "Dimensão do vetor do documento: 100\n"
     ]
    }
   ],
   "source": [
    "# 5. Obter um vetor para o documento inteiro (opcional)\n",
    "# Uma maneira simples é tirar a média dos vetores de todas as palavras no documento\n",
    "import numpy as np\n",
    "\n",
    "def get_document_vector(model, text_list):\n",
    "    \"\"\"Calcula a média dos vetores das palavras no documento.\"\"\"\n",
    "    vectors = [model.wv[word] for word in text_list if word in model.wv]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "document_vector1 = get_document_vector(model1, processed_text1[0])\n",
    "print(f\"\\nVetor do documento (média dos vetores das palavras no texto 1):\\n{document_vector}\")\n",
    "print(f\"Dimensão do vetor do documento: {len(document_vector)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "601c36fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vetor do documento (média dos vetores das palavras no texto 2):\n",
      "[-5.29617537e-04  7.70135957e-04  1.39765078e-04  5.55878505e-04\n",
      "  1.42321631e-04 -1.16600061e-03  9.01830033e-04  2.05199886e-03\n",
      " -1.16814894e-03 -9.79513396e-04  1.56985589e-05 -1.08516915e-03\n",
      " -1.95458531e-04  4.01439203e-04  1.86275953e-04 -8.66785937e-04\n",
      "  8.89081159e-04 -5.15104795e-04 -7.57790811e-04 -2.33665644e-03\n",
      "  4.96209774e-04 -5.06648612e-05  1.57977268e-03 -4.56533424e-04\n",
      " -5.18668501e-04  1.08133550e-04 -7.51901825e-04 -1.12117355e-04\n",
      " -1.04805198e-03  4.73991444e-04  1.21922058e-03 -4.10431356e-04\n",
      "  3.27792513e-04 -1.18291273e-03 -5.10402489e-04  1.10493146e-03\n",
      "  3.05078225e-04 -5.91725286e-04 -7.75955676e-04 -1.22505077e-03\n",
      "  7.32098764e-04 -1.06851745e-03 -7.99787405e-04  3.28298105e-04\n",
      "  1.01290445e-03 -3.64711828e-04 -5.24596253e-04 -7.92251783e-04\n",
      "  5.50458732e-04  3.76215059e-04  7.38859992e-04 -8.77296785e-04\n",
      " -1.22024561e-04 -1.80589574e-04 -8.84129026e-04  2.98518193e-04\n",
      "  2.83718051e-04 -3.45357810e-04 -1.01696968e-03  1.55035086e-04\n",
      "  1.24726766e-05 -2.72335397e-04  1.18278386e-03 -5.16385539e-04\n",
      " -1.25280046e-03  1.23966020e-03  6.06619054e-04  1.17622223e-03\n",
      " -1.72632304e-03  7.21777731e-04 -3.39108577e-04  9.40908387e-04\n",
      "  9.39843943e-04  6.15152530e-04  9.45625361e-04  4.80679009e-04\n",
      "  1.57875213e-04  3.92663613e-04 -6.86693005e-04  3.28015485e-05\n",
      " -9.07906389e-04 -4.92333376e-04 -5.35424391e-04  9.04373534e-04\n",
      " -6.09389623e-04 -6.77109987e-04  7.26927305e-04  5.68863412e-04\n",
      "  9.76536656e-04  1.42358680e-04  1.02443621e-03  7.47221697e-04\n",
      " -1.03205937e-04 -3.24371242e-04  1.64540880e-03  6.27100351e-04\n",
      "  6.44801243e-04 -9.34943033e-04  4.53454384e-04 -1.51244836e-04]\n",
      "Dimensão do vetor do documento: 100\n"
     ]
    }
   ],
   "source": [
    "document_vector2 = get_document_vector(model2, processed_text2[0])\n",
    "print(f\"\\nVetor do documento (média dos vetores das palavras no texto 2):\\n{document_vector}\")\n",
    "print(f\"Dimensão do vetor do documento: {len(document_vector)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "88f11b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similaridade entre o Vetor 1 e o Vetor 2: 0.9150\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# A função 'cosine' da SciPy calcula a distância do cosseno.\n",
    "# A similaridade do cosseno é (1 - distância do cosseno).\n",
    "distancia_1_2 = cosine(document_vector1, document_vector2)\n",
    "similaridade_1_2 = 1 - distancia_1_2\n",
    "\n",
    "print(f\"Similaridade entre o Vetor 1 e o Vetor 2: {similaridade_1_2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ae0376",
   "metadata": {},
   "source": [
    "Gesim se provou vetorizar bem\n",
    "\n",
    "# teste de tempo e acuracia faltam\n",
    "## rever o modelo utilizado e pensar em qual é melhor !! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4551edb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e10d38d",
   "metadata": {},
   "source": [
    "### Hugging Face:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6065412b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sentence_transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sentence_transformers'"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9aeecce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f6b5b48352a4c85ab5f8d4cb2b7c5a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\CursoDatascience\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Júlia & Bruno\\.cache\\huggingface\\hub\\models--sentence-transformers--paraphrase-multilingual-mpnet-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8f64cd263164bfa9cd76a47e15263d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8179b9c124e42699fa619c639cea10c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "253c5de608724faeb769d7e57480b3dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e95edb6889674cf992d9f6ac9440a785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/723 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9112d233629146509e5f1c881fc2ff82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23f9cdaddab94a92a63a253e3e46acee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/402 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d19dedd68a64f1280f8e4d25fadcbd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b94189871b5f4200b217c29884dbff21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c517daef31b1475fafb16ddacb322298",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adf97da9314a4c7bb5ba7d4bdbe6d589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Escolha e carregue um modelo de embedding da Hugging Face\n",
    "# Existem centenas de modelos. \"all-MiniLM-L6-v2\" é um modelo pequeno e rápido,\n",
    "# e \"paraphrase-multilingual-mpnet-base-v2\" é bom para vários idiomas, incluindo o português.\n",
    "# Para este exemplo, usaremos o modelo multilingual.\n",
    "model_name = 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'\n",
    "model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00d1ac05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Ler o arquivo de texto\n",
    "file_path1 = 'textos/Texto_grande1.txt'\n",
    "with open(file_path1, 'r', encoding='utf-8') as file:\n",
    "    text_data2 = file.read()\n",
    "    \n",
    "file_path2 = 'textos/Texto_grande2.txt'\n",
    "with open(file_path2, 'r', encoding='utf-8') as file:\n",
    "    text_data1 = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15597857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Gerar o vetor (embedding) para o texto completo\n",
    "# O modelo processa o texto inteiro e retorna um único vetor que representa o seu significado.\n",
    "document_embedding1 = model.encode(text_data1)\n",
    "document_embedding2 = model.encode(text_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08e8240f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dimensão do vetor do texto 1: (768,)\n",
      "Tipo do vetor do texto 1: <class 'numpy.ndarray'>\n",
      "\n",
      "Dimensão do vetor do texto 2: (768,)\n",
      "Tipo do vetor do texto 2: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#print(f\"Vetor do documento:\\n{document_embedding1}\")\n",
    "print(f\"\\nDimensão do vetor do texto 1: {document_embedding1.shape}\")\n",
    "print(f\"Tipo do vetor do texto 1: {type(document_embedding1)}\")\n",
    "\n",
    "print(f\"\\nDimensão do vetor do texto 2: {document_embedding2.shape}\")\n",
    "print(f\"Tipo do vetor do texto 2: {type(document_embedding2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec9977f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similaridade entre o Vetor 1 e o Vetor 2: 0.6213\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# A função 'cosine' da SciPy calcula a distância do cosseno.\n",
    "# A similaridade do cosseno é (1 - distância do cosseno).\n",
    "distancia_1_2 = cosine(document_embedding1, document_embedding2)\n",
    "similaridade_1_2 = 1 - distancia_1_2\n",
    "\n",
    "print(f\"Similaridade entre o Vetor 1 e o Vetor 2: {similaridade_1_2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d956dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de17d1f9",
   "metadata": {},
   "source": [
    "# trazer a acuracia o tempo, talvez trocar o modelo ! \n",
    "#### mas vetores não estão tão bons quanto o do gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbbb9cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1afcccaf",
   "metadata": {},
   "source": [
    "# 2. Comparação entre Elasticsearch vs GCP Vector Search\n",
    "\n",
    "### Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c574696",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e7c073e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Conectar ao Elasticsearch\n",
    "es = Elasticsearch(hosts=[\"http://localhost:9200\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b2bcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Definir o nome do índice e o tipo de campo para o vetor\n",
    "INDEX_NAME = \"documentos_vetorizados\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1b9280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Criar o índice com o mapeamento 'dense_vector'\n",
    "# Um vetor de 3 dimensões para este exemplo\n",
    "index_mapping = {\n",
    "    \"properties\": {\n",
    "        \"texto\": {\"type\": \"text\"},\n",
    "        \"vetor_embedding\": {\n",
    "            \"type\": \"dense_vector\",\n",
    "            \"dims\": 3  # Dimensão do vetor (neste caso, 3)\n",
    "        }\n",
    "    }\n",
    "}\n",
    "es.indices.create(index=INDEX_NAME, body={\"mappings\": index_mapping})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ed547a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Adicionar alguns documentos com seus vetores\n",
    "doc1 = {\"texto\": \"cachorro brincando\", \"vetor_embedding\": [0.1, 0.2, 0.7]}\n",
    "doc2 = {\"texto\": \"gato correndo\", \"vetor_embedding\": [0.6, 0.8, 0.1]}\n",
    "doc3 = {\"texto\": \"cão e gato\", \"vetor_embedding\": [0.2, 0.3, 0.6]}\n",
    "\n",
    "es.index(index=INDEX_NAME, id=1, body=doc1)\n",
    "es.index(index=INDEX_NAME, id=2, body=doc2)\n",
    "es.index(index=INDEX_NAME, id=3, body=doc3)\n",
    "\n",
    "# Aguardar um pouco para os documentos serem indexados\n",
    "es.indices.refresh(index=INDEX_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf162924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Definir um vetor de consulta para buscar documentos semelhantes\n",
    "# Este vetor representa \"cão brincando\" e é similar ao doc1 e doc3\n",
    "query_vector = [0.15, 0.25, 0.65]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae575e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Realizar a busca por similaridade usando k-NN\n",
    "search_body = {\n",
    "    \"query\": {\n",
    "        \"script_score\": {\n",
    "            \"query\": {\"match_all\": {}},\n",
    "            \"script\": {\n",
    "                \"source\": \"cosineSimilarity(params.query_vector, 'vetor_embedding') + 1.0\",\n",
    "                \"params\": {\"query_vector\": query_vector}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "response = es.search(index=INDEX_NAME, body=search_body, size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290c78db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Imprimir os resultados\n",
    "print(\"\\nResultados da busca por similaridade:\")\n",
    "for hit in response['hits']['hits']:\n",
    "    print(f\"ID: {hit['_id']}, Score: {hit['_score']:.4f}, Texto: {hit['_source']['texto']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10865a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df78eec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5f8ba0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
